{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6d1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_spam_data():\n",
    "    tarball_path_spam = Path(\"datasets/spam_data.tar.bz2\")\n",
    "    tarball_path_ham = Path(\"datasets/ham_data.tar.bz2\")\n",
    "    \n",
    "    if not tarball_path_spam.is_file() and not tarball_path_ham.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True , exist_ok=True)\n",
    "        \n",
    "        spam_url = \"https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\"\n",
    "        ham_url = \"https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\"\n",
    "        \n",
    "        urllib.request.urlretrieve(spam_url , tarball_path_spam)\n",
    "        with tarfile.open(tarball_path_spam) as tb:\n",
    "            tb.extractall(path=\"datasets\" , filter=\"data\")\n",
    "            \n",
    "        urllib.request.urlretrieve(ham_url , tarball_path_ham)\n",
    "        with tarfile.open(tarball_path_ham) as tb:\n",
    "            tb.extractall(path=\"datasets\" , filter=\"data\")\n",
    "            \n",
    "    return [Path(\"datasets/spam\") , Path(\"datasets/easy_ham\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa731d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dir , ham_dir = load_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17937a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_filenames = [f for f in sorted(spam_dir.iterdir()) if len(f.name) > 20]\n",
    "ham_filenames = [f for f in sorted(ham_dir.iterdir()) if len(f.name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399afac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of spam: 501\n",
      "Num of ham: 2551\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num of spam: {len(spam_filenames)}\")\n",
    "print(f\"Num of ham: {len(ham_filenames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e37f47",
   "metadata": {},
   "source": [
    "Using Python's email lib to decode the mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1819b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def get_email(filename):\n",
    "    mail_parser = email.parser.BytesParser(policy=email.policy.default)\n",
    "    with open(filename , \"rb\") as f:\n",
    "        return mail_parser.parse(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7119a",
   "metadata": {},
   "source": [
    "Decode all email files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ff6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails = [get_email(f) for f in spam_filenames]\n",
    "ham_emails = [get_email(f) for f in ham_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbfaeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(spam_emails[2]) #or print(\"spam_emails[2].get_content().strip()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7202069",
   "metadata": {},
   "source": [
    "Split data into train and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8454839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails , dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails)) #ham => 0 | spam => 1\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90dcce",
   "metadata": {},
   "source": [
    "Get an insight on the structure of the mails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28690444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<email.message.EmailMessage object at 0x12303ae10>, <email.message.EmailMessage object at 0x122f3edd0>]\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[10].get_payload()) #=> emails with more content (e.g. larger conversations, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cffb2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        multipart = \", \".join([get_email_structure(sub_email) for sub_email in payload])\n",
    "        return f\"multipart({multipart})\"\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b9f46c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipart(text/plain, application/pgp-signature)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_email_structure(ham_emails[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716c8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f3c89",
   "metadata": {},
   "source": [
    "Compute the most common structures to get an idea on how typical ham and spam mails are constructed/structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccdb00d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2453),\n",
       " ('multipart(text/plain, application/pgp-signature)', 72),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae08817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 222),\n",
       " ('text/html', 181),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 19),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee91b3",
   "metadata": {},
   "source": [
    "First conclusion: The structure is a useful information. For example ham mails mostly consist of text only and sometimes use pgp signature, while spam almost always consists of html and no pgp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e807524",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3c39b",
   "metadata": {},
   "source": [
    "1. Convert HTML Tags into plain text using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c945bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def email_html_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        clean_text = soup.get_text(separator=' ' , strip=True)\n",
    "        return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f1e827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<center>\n",
      "<h1>\n",
      "<b><font face=\"Arial Black\"><font color=\"#0000FF\"><font size=+2>&nbsp;\n",
      "Free Personal and Business Grants</font></font></font></b></h1></center>\n",
      "\n",
      "<p>&nbsp;\n",
      "<center><table BORDER=0 CELLSPACING=0 CELLPADDING=10 WIDTH=\"419\" BGCOLOR=\"#0000FF\" >\n",
      "<tr>\n",
      "<td WIDTH=\"397\" BGCOLOR=\"#FFFF00\">\n",
      "<center>\n",
      "<h2>\n",
      "<font face=\"Arial Narrow\">\" Qualify for <u>at least</u> $25,000 in fre ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [mail for mail in X_train[y_train == 1] if get_email_structure(mail) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:400], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b7d255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Personal and Business Grants \" Qualify for at least $25,000 in free\n",
      "grants money - Guaranteed!  ...\n"
     ]
    }
   ],
   "source": [
    "print(email_html_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e1d8b",
   "metadata": {},
   "source": [
    "Works good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be67ef3",
   "metadata": {},
   "source": [
    "We will remove URLs and replace them with the word 'URL', using urlextract lib. Here: Prepare the lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8c16fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
     ]
    }
   ],
   "source": [
    "#%pip install -q -U urlextract #installing urlextract if not available\n",
    "\n",
    "import urlextract\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "\n",
    "#for testing:\n",
    "some_text = \"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"\n",
    "print(str(url_extractor.find_urls(some_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96e1e3",
   "metadata": {},
   "source": [
    "We will reduce the words to their stem/root form (stemming). Necessary for text analysis.\n",
    "Using  Natural Language Toolkit (NLTK) for this task. Here: Just prepare the stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85599c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2c6f2",
   "metadata": {},
   "source": [
    "Now, lets put everything togeter and create a transformer that that transforms emails to word counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f179442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.base import BaseEstimator , TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator , TransformerMixin):\n",
    "    def __init__(self , strip_headers=True , lower_case=True ,\n",
    "                remove_punctuation=True , replace_urls=True ,\n",
    "                replace_numbers=True , stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "        \n",
    "        \n",
    "    def fit(self , X , y=None): #processing here not necessary, we only transform\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self , X , y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_html_to_text(email) or \"\"\n",
    "            \n",
    "            #set lower case if demanded\n",
    "            if self.lower_case: text = text.lower()\n",
    "                \n",
    "            #replace urls in text with constant string \" URL \", if demanded\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url) , reverse=True)\n",
    "                for url in urls: text = text.replace(url , \" URL \")\n",
    "                    \n",
    "            #replace numbers with constant string \"NUMBER\", if demanded\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?' , \"NUMBER\" , text)\n",
    "            \n",
    "            #remove punctuation if demanded\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+' , ' ' , text , flags=re.M)\n",
    "                \n",
    "            #start counting words and use stemming if requested\n",
    "            word_counts = Counter(text.split()) #split text along spaces between words (not sufficient for e.g. Chinese language)\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word , count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            \n",
    "            X_transformed.append(word_counts)\n",
    "            \n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b3f6a",
   "metadata": {},
   "source": [
    "Test our brandnew transformer on a few emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a53e32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "#X_few_wordcounts #remove comment for printout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30887502",
   "metadata": {},
   "source": [
    "Now we need to transform the word counts into vectors. We build another transformer for this task. Its fit() method will build an ordered list of the most common words and its transform() method will use this list to convert the word counts into vectors. The output will be a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a57e3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator , TransformerMixin):\n",
    "    def __init__(self , vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "        \n",
    "    def fit(self , X , y=None):\n",
    "        total_count = Counter()\n",
    "        \n",
    "        for word_count in X:\n",
    "            for word , count in word_count.items():\n",
    "                total_count[word] += min(count , 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index , (word , count) in enumerate(most_common)}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self , X , y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row , word_count in enumerate(X):\n",
    "            for word , count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word , 0))\n",
    "                data.append(count)\n",
    "                \n",
    "        return csr_matrix((data , (rows , cols)) , \n",
    "                        shape=(len(X) , self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93fb7b",
   "metadata": {},
   "source": [
    "Also test this transformer on some word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47985b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 51,   2,   0,   2,   4,   1,   1,   3,   2,   2,   2],\n",
       "       [116,   8,   7,   5,   3,   5,   3,   2,   3,   1,   2],\n",
       "       [ 46,   3,   5,   0,   0,   1,   1,   0,   0,   2,   0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09e63d",
   "metadata": {},
   "source": [
    "How to read this sparse matrix: The second row stands for the second mail. The 116 means this mail contains 116 words that are not part of the vocabulary. The 8 next to it means that the first word in the vocabuliry is present 8 times in that particular email. The 7 next to it means the second word is present 7 times, and so on. The vocabulary can be seen via the vocabulary_ internal variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4df47811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'number': 2,\n",
       " 'the': 3,\n",
       " 'it': 4,\n",
       " 'to': 5,\n",
       " 'that': 6,\n",
       " 't': 7,\n",
       " 'look': 8,\n",
       " 'at': 9,\n",
       " 'a': 10}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ad5dd",
   "metadata": {},
   "source": [
    "So coming back to the sparse matrix from above: The second mail has 116 times the word 'I' in it. The word 'number' is present 8 times, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8501f6",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb463c",
   "metadata": {},
   "source": [
    "We are now ready to train the spam classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f820baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\" , EmailToWordCounterTransformer()) ,\n",
    "    (\"wordcount_to_vector\" , WordCounterToVectorTransformer())\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f385e88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9877094471190412"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(max_iter=1000 , random_state=42)\n",
    "score = cross_val_score(log_clf , X_train_transformed , y_train , cv=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d325c33",
   "metadata": {},
   "source": [
    "98,7% is not bad for a start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8835ad6",
   "metadata": {},
   "source": [
    "## Measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72be22ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  91.67%\n",
      "Recall:  95.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score , recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test) #do not fit on test data!\n",
    "\n",
    "log_clf = LogisticRegression(max_iter=1000 , random_state=42)\n",
    "log_clf.fit(X_train_transformed , y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test , y_pred): .2%}\")\n",
    "print(f\"Recall: {recall_score(y_test , y_pred): .2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a9616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
